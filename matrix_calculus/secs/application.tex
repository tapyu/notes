\section{Unconstrained optimization algorithms}

In this section, we use Matrix Calculus to derive unconstrained optimization algorithms. The key idea is to show how Matrix Calculus allows us to conceive simple and elegant expressions, without resorting to nonmatrix notation. However, we are not interested in the theoretical aspects of such algorithms, e.g., necessary and sufficient conditions for unconstrained minimization, Lipschitz continuity, Kantorovich theorem, etc. For these topics, the reader is encouraged to consult \cite{dennisNumericalMethodsUnconstrained1996}.

\subsection{Taylor Series}

\obs{TODO: see Tasinaffo material in ``Matrix Calculus/Denominator/'' directory on Google Drive}

\subsection{The Steepest Descent algorithm}

\obs{TODO: \cite{dinizAdaptiveFilteringAlgorithms2002} \cite{haykinNeuralNetworksLearning2009}}

\subsection{Newton's Method}

\obs{TODO: \cite{haykinNeuralNetworksLearning2009}}

\subsection{The Gauss-Newton Method}

\obs{TODO: \cite{haykinNeuralNetworksLearning2009}}

\subsection{The Levenberg-Marquardt Method.}

\obs{See \cite{haykinNeuralNetworksLearning2009}, in the unconstrained optimization methods; \cite{pacelliDataBasedEstimationPowerLaw2024} and the references therein; \cite{madsenMethodsNonLinearLeast2004}}